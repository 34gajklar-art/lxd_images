name: Simplestreams

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Install Dependencies
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq unzip curl

      - name: Initialize and Get Releases
        id: init_releases
        env:
          REPO: ${{ github.repository }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Fetch all releases from GitHub API
          releases=$(curl -s "https://api.github.com/repos/$REPO/releases")
          # Find or create the 'processed' release for storing processed assets
          processed_release_id=$(echo "$releases" | jq -r '.[] | select(.tag_name=="processed") | .id')
          if [ -z "$processed_release_id" ]; then
            echo "Creating 'processed' release..."
            resp=$(curl -s -X POST -H "Authorization: token $GH_TOKEN" \
                     -d '{"tag_name":"processed","name":"Processed Assets","draft":false,"prerelease":false}' \
                     "https://api.github.com/repos/$REPO/releases")
            processed_release_id=$(echo "$resp" | jq -r '.id')
            if [ -z "$processed_release_id" ] || [ "$processed_release_id" = "null" ]; then
              echo "Error: Failed to create 'processed' release." >&2
              echo "Response: $resp" >&2
              exit 1
            fi
          fi
          echo "Processed release ID: $processed_release_id"
          # Save processed release ID for later steps
          echo "release_id=$processed_release_id" >> $GITHUB_OUTPUT
          # Save all releases JSON to file for parsing
          printf "%s" "$releases" > releases.json

      - name: Clear Old Processed Assets
        env:
          REPO: ${{ github.repository }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PROCESSED_ID: ${{ steps.init_releases.outputs.release_id }}
        run: |
          # Delete all assets in the processed release to avoid duplicates
          assets=$(curl -s "https://api.github.com/repos/$REPO/releases/$PROCESSED_ID/assets" | jq -r '.[].id')
          for asset_id in $assets; do
            echo "Deleting asset $asset_id from processed release..."
            curl -s -X DELETE -H "Authorization: token $GH_TOKEN" \
                 "https://api.github.com/repos/$REPO/releases/assets/$asset_id" > /dev/null
          done

      - name: Process and Upload Images
        env:
          REPO: ${{ github.repository }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PROCESSED_ID: ${{ steps.init_releases.outputs.release_id }}
        run: |
          mkdir -p work && cd work
          echo "[]" > processed_assets.json
          # Filter all release assets (exclude 'processed' release and non-matching names)
          echo "Gathering image ZIP assets..."
          jq -r '.[] | select(.tag_name != "processed") | .assets[] | select(.name | test("^[a-z0-9]+_[0-9a-z\\.\\-]+_[a-z0-9]+_[a-z0-9]+_[a-z]+\\.zip$")) | @base64' ../releases.json > to_process.txt
          while IFS= read -r asset_b64; do
            [ -z "$asset_b64" ] && continue
            asset=$(echo "$asset_b64" | base64 -d)
            name=$(echo "$asset" | jq -r '.name')
            url=$(echo "$asset" | jq -r '.browser_download_url')
            # Parse the file name into components (os_version_codename_arch_variant.zip)
            if [[ "$name" =~ ^([a-z0-9]+)_([0-9a-z\.\-]+)_([a-z0-9]+)_([a-z0-9]+)_([a-z]+)\.zip$ ]]; then
              os="${BASH_REMATCH[1]}"
              version="${BASH_REMATCH[2]}"
              codename="${BASH_REMATCH[3]}"
              arch="${BASH_REMATCH[4]}"
              variant="${BASH_REMATCH[5]}"
            else
              echo "Skipping unmatched asset name: $name"
              continue
            fi
            echo "Processing $os $version $arch ($variant)..."
            # Create a temp dir for extraction
            temp_dir="${os}_${version}_${arch}_${variant}"
            mkdir -p "$temp_dir" && cd "$temp_dir"
            # Download and unzip the image archive
            curl -L -s --retry 3 --retry-delay 5 -o image.zip "$url"
            unzip -q image.zip || { echo "Failed to extract $name"; cd ..; rm -rf "$temp_dir"; continue; }
            # Ensure required files exist
            if [ ! -f rootfs.squashfs ] || [ ! -f lxd.tar.xz ]; then
              echo "Required files missing in $name; skipping."
              cd .. && rm -rf "$temp_dir"
              continue
            fi
            # Rename extracted files with standardized names
            rootfs_file="${os}-${version}-${arch}-${variant}-rootfs.squashfs"
            meta_file="${os}-${version}-${arch}-${variant}-lxd.tar.xz"
            mv rootfs.squashfs "$rootfs_file"
            mv lxd.tar.xz "$meta_file"
            # Calculate file size and SHA-256 checksum
            rootfs_size=$(stat -c%s "$rootfs_file")
            meta_size=$(stat -c%s "$meta_file")
            rootfs_sha256=$(sha256sum "$rootfs_file" | cut -d ' ' -f1)
            meta_sha256=$(sha256sum "$meta_file" | cut -d ' ' -f1)
            # Upload rootfs and metadata to the processed release
            echo "Uploading $rootfs_file..."
            rootfs_resp=$(curl -s -X POST -H "Authorization: token $GH_TOKEN" \
                          -H "Content-Type: application/octet-stream" \
                          --data-binary @"$rootfs_file" \
                          "https://uploads.github.com/repos/$REPO/releases/$PROCESSED_ID/assets?name=$rootfs_file")
            echo "Uploading $meta_file..."
            meta_resp=$(curl -s -X POST -H "Authorization: token $GH_TOKEN" \
                        -H "Content-Type: application/octet-stream" \
                        --data-binary @"$meta_file" \
                        "https://uploads.github.com/repos/$REPO/releases/$PROCESSED_ID/assets?name=$meta_file")
            rootfs_url=$(echo "$rootfs_resp" | jq -r '.browser_download_url // empty')
            meta_url=$(echo "$meta_resp" | jq -r '.browser_download_url // empty')
            if [[ -n "$rootfs_url" && -n "$meta_url" ]]; then
              echo "Uploaded: $rootfs_file and $meta_file"
              # Append metadata to processed_assets.json
              jq --arg os "$os" --arg version "$version" --arg codename "$codename" \
                 --arg arch "$arch" --arg variant "$variant" \
                 --arg rootfs_url "$rootfs_url" --arg rootfs_sha256 "$rootfs_sha256" --argjson rootfs_size "$rootfs_size" \
                 --arg meta_url "$meta_url" --arg meta_sha256 "$meta_sha256" --argjson meta_size "$meta_size" \
                 '. += [{
                    os: $os,
                    version: $version,
                    codename: $codename,
                    arch: $arch,
                    variant: $variant,
                    rootfs_url: $rootfs_url,
                    rootfs_sha256: $rootfs_sha256,
                    rootfs_size: $rootfs_size,
                    meta_url: $meta_url,
                    meta_sha256: $meta_sha256,
                    meta_size: $meta_size
                 }]' ../processed_assets.json > ../processed_assets.tmp && mv ../processed_assets.tmp ../processed_assets.json
            else
              echo "Failed to upload one or both files for $name"
            fi
            cd .. && rm -rf "$temp_dir"
          done
          mv ../processed_assets.json /tmp/processed_assets.json

      - name: Generate Simplestreams JSON
        run: |
          mkdir -p pages/streams/v1
          # Prepare updated timestamp in RFC 2822 format (e.g. Fri, 06 Jun 2025 15:30:00 +0000)
          updated=$(date -u +"%a, %d %b %Y %T +0000")
          # Generate images (download) JSON
          jq -n --slurpfile imgs /tmp/processed_assets.json --arg updated "$updated" '{
              content_id: "spiritlhl:images:download",
              datatype: "image-downloads",
              format: "products:1.0",
              updated: $updated,
              products: ($imgs[0] | reduce .[] as $item ({}; . + {
                ($item.os | ascii_downcase) as $os
                | ($item.variant | ascii_downcase) as $var
                | ($item.version) as $ver
                | ($item.arch) as $arch
                | ($item.os + ":" + $ver + ":" + $arch + ":" + $var | "spiritlhl:" + .): {
                    os: $os,
                    release: ($item.codename // $ver),
                    release_codename: ($item.codename // $ver),
                    release_title: (($os|ascii_upcase) + " " + $ver + ($item.codename? (" (" + $item.codename + ")") : "")),
                    version: $ver,
                    arch: $arch,
                    variant: $var,
                    supported: true,
                    versions: {
                      "'$(date +%Y%m%d)'": {
                        items: {
                          "rootfs.squashfs": {
                            ftype: "squashfs",
                            path: $item.rootfs_url,
                            sha256: $item.rootfs_sha256,
                            size: ($item.rootfs_size | tonumber)
                          },
                          "lxd.tar.xz": {
                            ftype: "metadata",
                            path: $item.meta_url,
                            sha256: $item.meta_sha256,
                            size: ($item.meta_size | tonumber)
                          }
                        }
                      }
                    }
                }
              }))
          }' > pages/streams/v1/images.json
          # Generate index JSON pointing to the images JSON
          jq -n --arg updated "$updated" --slurpfile imgs /tmp/processed_assets.json '{
              format: "index:1.0",
              updated: $updated,
              index: {
                "spiritlhl:images:download": {
                  datatype: "image-downloads",
                  path: "streams/v1/images.json",
                  format: "products:1.0",
                  updated: $updated,
                  products: ($imgs[0] | map("spiritlhl:" + .os + ":" + .version + ":" + .arch + ":" + .variant))
                }
              }
          }' > pages/streams/v1/index.json
          # Optionally, set up CNAME or other pages content if needed
          echo "lxdimages.spiritlhl.net" > pages/CNAME
